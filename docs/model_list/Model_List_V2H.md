# Model list for RZ/V2H

Below is a list of AI models that Renesas has verified for conversion with the DRP-AI TVM[^1] and actual operation on an evaluation board.

| Item                       | RZ/V2H                    |
| -------------------------- | ------------------------- |
| DRP-AI TVM[^1]             | v2.4.0 / v2.5.0 [^2]      |
| Evaluation Board           | RZ/V2H EVK                |
| DRP-AI Translator          | i8 v1.03 / i8 v1.0.4 [^2] |
| Linux Package[^3]          | AI SDK v5.00              |
| DRP-AI Support Package[^3] | AI SDK v5.00              |

| AI model                                                                                                                               | Input Shape | Task                         | Format               | Inference time<br>(CPU only) | Inference time<br>(CPU+DRP-AI) |
| -------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ---------------------------- | -------------------- | ---------------------------- | ------------------------------ |
| [ResNet18-v1](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet18-v1-7.onnx)                | (224,224)   | Classification               | ONNX                 | 4450ms                       | 2ms                            |
| [ResNet18-v2](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet18-v2-7.onnx)                | (224,224)   | Classification               | ONNX                 | 4448ms                       | 5ms                            |
| [ResNet34-v1](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet34-v1-7.onnx)                | (224,224)   | Classification               | ONNX                 | 10041ms                      | 4ms                            |
| [ResNet34-v2](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet34-v2-7.onnx)                | (224,224)   | Classification               | ONNX                 | 10048ms                      | 8ms                            |
| [ResNet50-v1](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet50-v1-7.onnx)                | (224,224)   | Classification               | ONNX                 | 8936ms                       | 4ms                            |
| [ResNet50-v2](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet50-v2-7.onnx)                | (224,224)   | Classification               | ONNX                 | 9547ms                       | 15ms                           |
| [ResNet101-v1](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet101-v1-7.onnx)              | (224,224)   | Classification               | ONNX                 | 19318ms                      | 7ms                            |
| [ResNet101-v2](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/model/resnet101-v2-7.onnx)              | (224,224)   | Classification               | ONNX                 | 19822ms                      | 26ms                           |
| [MobileNetV2](https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/model/mobilenetv2-7.onnx)             | (224,224)   | Classification               | ONNX                 | 517ms                        | 1ms                            |
| [SqueezeNet1.1-7](https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/model/squeezenet1.1-7.onnx)      | (224,224)   | Classification               | ONNX                 | 698ms                        | 3ms                            |
| [DenseNet9](https://github.com/onnx/models/blob/main/validated/vision/classification/densenet-121/model/densenet-9.onnx)               | (224,224)   | Classification               | ONNX                 | 5186ms                       | 26ms                           |
| [YOLOv2](https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov2-coco/model/yolov2-coco-9.onnx) | (416,416)   | Object Detection             | ONNX                 | 41648ms                      | 14ms                           |
| [YOLOv3](./how_to_convert/How_to_convert_yolov3_onnx_model_V2H.md)                                                                     | (416,416)   | Object Detection             | ONNX                 | 4208ms                       | 27ms                           |
| [YOLOv5s](./how_to_convert/How_to_convert_yolov5_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 1454ms                       | 19ms                           |
| [YOLOv5m](./how_to_convert/How_to_convert_yolov5_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 4062ms                       | 33ms                           |
| [YOLOv5l](./how_to_convert/How_to_convert_yolov5_onnx_models_V2H.md)[^2] [^4]                                                          | (640,640)   | Object Detection             | ONNX                 | 7420ms                       | 45ms                           |
| [YOLOv5x](./how_to_convert/How_to_convert_yolov5_onnx_models_V2H.md)[^2] [^4]                                                          | (640,640)   | Object Detection             | ONNX                 | 13545ms                      | 81ms                           |
| [YOLOv6n](./how_to_convert/How_to_convert_yolov6_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 674ms                        | 10ms                           |
| [YOLOv6s](./how_to_convert/How_to_convert_yolov6_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 2404ms                       | 16ms                           |
| [YOLOv6m](./how_to_convert/How_to_convert_yolov6_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 4967ms                       | 38ms                           |
| [YOLOv6l](./how_to_convert/How_to_convert_yolov6_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 9109ms                       | 49ms                           |
| [YOLOv7](./how_to_convert/How_to_convert_yolov7_onnx_models_V2H.md)[^2]                                                                | (640,640)   | Object Detection             | ONNX                 | 7182ms                       | 49ms                           |
| [YOLOv7x](./how_to_convert/How_to_convert_yolov7_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 12301ms                      | 71ms                           |
| [YOLOv7-w6](./how_to_convert/How_to_convert_yolov7_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 5794ms                       | 43ms                           |
| [YOLOv7-e6](./how_to_convert/How_to_convert_yolov7_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 8649ms                       | 65ms                           |
| [YOLOv7-d6](./how_to_convert/How_to_convert_yolov7_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 11775ms                      | 78ms                           |
| [YOLOv7-e6e](./how_to_convert/How_to_convert_yolov7_onnx_models_V2H.md)[^2]                                                            | (640,640)   | Object Detection             | ONNX                 | 13933ms                      | 96ms                           |
| [YOLOv8n](./how_to_convert/How_to_convert_yolov8_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 708ms                        | 19ms                           |
| [YOLOv8s](./how_to_convert/How_to_convert_yolov8_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 2017ms                       | 28ms                           |
| [YOLOv8m](./how_to_convert/How_to_convert_yolov8_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 5676ms                       | 50ms                           |
| [YOLOv8l](./how_to_convert/How_to_convert_yolov8_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 9599ms                       | 68ms                           |
| [YOLOv8x](./how_to_convert/How_to_convert_yolov8_onnx_models_V2H.md)[^2]                                                               | (640,640)   | Object Detection             | ONNX                 | 14925ms                      | 100ms                          |
| [YOLOv9-t](./how_to_convert/How_to_convert_yolov9_onnx_models_V2H.md)[^2]                                                              | (640,640)   | Object Detection             | ONNX                 | 777ms                        | 23ms                           |
| [YOLOv9-s](./how_to_convert/How_to_convert_yolov9_onnx_models_V2H.md)[^2]                                                              | (640,640)   | Object Detection             | ONNX                 | 2084ms                       | 29ms                           |
| [YOLOv9-m](./how_to_convert/How_to_convert_yolov9_onnx_models_V2H.md)[^2]                                                              | (640,640)   | Object Detection             | ONNX                 | 6326ms                       | 58ms                           |
| [YOLOv9-c](./how_to_convert/How_to_convert_yolov9_onnx_models_V2H.md)[^2] [^4]                                                         | (640,640)   | Object Detection             | ONNX                 | 6540ms                       | 74ms                           |
| [YOLOv10n](./how_to_convert/How_to_convert_yolov10_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 813ms                        | 51ms                           |
| [YOLOv10s](./how_to_convert/How_to_convert_yolov10_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 2043ms                       | 82ms                           |
| [YOLOv10m](./how_to_convert/How_to_convert_yolov10_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 5100ms                       | 106ms                          |
| [YOLOv10b](./how_to_convert/How_to_convert_yolov10_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 6433ms                       | 117ms                          |
| [YOLOv10l](./how_to_convert/How_to_convert_yolov10_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 8207ms                       | 126ms                          |
| [YOLOv10x](./how_to_convert/How_to_convert_yolov10_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 11947ms                      | 172ms                          |
| [YOLOv11n](./how_to_convert/How_to_convert_yolov11_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 796ms                        | 52ms                           |
| [YOLOv11s](./how_to_convert/How_to_convert_yolov11_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 2170ms                       | 86ms                           |
| [YOLOv11m](./how_to_convert/How_to_convert_yolov11_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 5373ms                       | 120ms                          |
| [YOLOv11l](./how_to_convert/How_to_convert_yolov11_onnx_models_V2H.md)[^2]                                                             | (640,640)   | Object Detection             | ONNX                 | 6961ms                       | 170ms                          |
| [YOLOX_s](./how_to_convert/How_to_convert_yolox_onnx_models_V2H.md)[^2]                                                                | (640,640)   | Object Detection             | ONNX                 | 2004ms                       | 16ms                           |
| [YOLOX_m](./how_to_convert/How_to_convert_yolox_onnx_models_V2H.md)[^2]                                                                | (640,640)   | Object Detection             | ONNX                 | 5760ms                       | 33ms                           |
| [YOLOX_l](./how_to_convert/How_to_convert_yolox_onnx_models_V2H.md)[^2]                                                                | (640,640)   | Object Detection             | ONNX                 | 9141ms                       | 47ms                           |
| [YOLOX_x](./how_to_convert/How_to_convert_yolox_onnx_models_V2H.md)[^2]                                                                | (640,640)   | Object Detection             | ONNX                 | 16446ms                      | 87ms                           |
| [EdgeYOLO_Tiny](./how_to_convert/How_to_convert_edgeyolo_models_onnx_V2H.md)[^2]                                                       | (640,640)   | Object Detection             | ONNX                 | 2761ms                       | 55ms                           |
| [EdgeYOLO_S](./how_to_convert/How_to_convert_edgeyolo_models_onnx_V2H.md)[^2]                                                          | (640,640)   | Object Detection             | ONNX                 | 3672ms                       | 55ms                           |
| [EdgeYOLO_M](./how_to_convert/How_to_convert_edgeyolo_models_onnx_V2H.md)[^2]                                                          | (640,640)   | Object Detection             | ONNX                 | 5180ms                       | 41ms                           |
| [EdgeYOLO](./how_to_convert/How_to_convert_edgeyolo_models_onnx_V2H.md)[^2]                                                            | (640,640)   | Object Detection             | ONNX                 | 8045ms                       | 47ms                           |
| [HRNet](./how_to_convert/How_to_convert_hrnet_onnx_model_V2H.md)                                                                       | (256,192)   | Body Keypiont 2D             | ONNX                 | 5386ms                       | 9ms                            |
| [YoloX_S-Pose](./how_to_convert/Translator_models.md) [^2]                                                                             | (640,640)   | Multi person pose estimation | ONNX                 | 2558ms                       | 18ms                           |
| [Yolo11_n-Pose](./how_to_convert/How_to_convert_yolo11pose_onnx_models_V2H.md) [^2]                                                    | (640,640)   | Multi person pose estimation | ONNX                 | 936ms                        | 48ms                           |
| [Yolo11_S-Pose](./how_to_convert/How_to_convert_yolo11pose_onnx_models_V2H.md)  [^2]                                                   | (640,640)   | Multi person pose estimation | ONNX                 | 2360ms                       | 80ms                           |
| [Yolo11_M-Pose](./how_to_convert/How_to_convert_yolo11pose_onnx_models_V2H.md)[^2]                                                     | (640,640)   | Multi person pose estimation | ONNX                 | 5712ms                       | 110ms                          |
| [Yolo11_L-Pose](./how_to_convert/How_to_convert_yolo11pose_onnx_models_V2H.md)    [^2]                                                 | (640,640)   | Multi person pose estimation | ONNX                 | 7199ms                       | 160ms                          |
| [UNET-mobilenetV2](./how_to_convert/How_to_convert_UNET_onnx_models_V2H.md)    [^2]                                                    | (256,256)   | Segmentation                 | ONNX                 | 739ms                        | 13ms                           |
| [ResNet18](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                                  | (224,224)   | Classification               | PyTorch              | 4451ms                       | 3ms                            |
| [ResNet34](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                                  | (224,224)   | Classification               | PyTorch              | 10019ms                      | 4ms                            |
| [ResNet50](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                                  | (224,224)   | Classification               | PyTorch              | 9548ms                       | 4ms                            |
| [ResNet101](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                                 | (224,224)   | Classification               | PyTorch              | 19857ms                      | 7ms                            |
| [ResNeXt-50-32x4d](./how_to_convert/How_to_convert_torchvision_model_V2H_exceptional_case.md)                                          | (224,224)   | Classification               | PyTorch              | 7625ms                       | 176ms                          |
| [MobileNetV2](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                               | (224,224)   | Classification               | PyTorch              | 447ms                        | 1ms                            |
| [SqueezeNet1_1](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                             | (224,224)   | Classification               | PyTorch              | 720ms                        | 2ms                            |
| [DenseNet-121](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                              | (224,224)   | Classification               | PyTorch              | 5411ms                       | 27ms                           |
| [DenseNet-161](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                              | (224,224)   | Classification               | PyTorch              | 11355ms                      | 58ms                           |
| [GoogleNet](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                                 | (224,224)   | Classification               | PyTorch              | 2680ms                       | 15ms                           |
| [MnasNet0_5](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                                | (224,224)   | Classification               | PyTorch              | 167ms                        | 2ms                            |
| [DeepLabv3-resnet50](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                        | (224,224)   | Segmentation                 | PyTorch              | 63707ms                      | 27ms                           |
| [DeepLabv3-resnet101](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                       | (224,224)   | Segmentation                 | PyTorch              | 97060ms                      | 32ms                           |
| [FCN_resnet101](./how_to_convert/How_to_convert_torchvision_models_V2H.md)                                                             | (224,224)   | Segmentation                 | PyTorch              | 66557ms                      | 29ms                           |
| [DeepPose](./how_to_convert/How_to_convert_mmpose_models_V2H.md)                                                                       | (256,192)   | Body Keypoint 2D             | PyTorch              | 6118ms                       | 4ms                            |
| [HRNetV2](./how_to_convert/How_to_convert_mmpose_models_V2H.md)                                                                        | (256,192)   | Face Detection 2D            | PyTorch              | 2731ms                       | 8ms                            |
| [HRNetV2 DarkPose](./how_to_convert/How_to_convert_mmpose_models_V2H.md)                                                               | (256,192)   | Face Detection 2D            | PyTorch              | 2750ms                       | 8ms                            |
| [Monodepth2 mono_640x192 encoder](./how_to_convert/How_to_convert_monodepth2_model_V2H.md)                                             | (640,192)   | Depth                        | PyTorch              | 7920ms                       | 18ms                           |
| [SC-Depth resnet18_depth_256 dispnet](./how_to_convert/How_to_convert_sc_depth_resnet_models_V2H.md)                                   | (832,256)   | Depth                        | PyTorch              | 17539ms                      | 313ms                          |
| [SC-Depth resnet50_depth_256 dispnet](./how_to_convert/How_to_convert_sc_depth_resnet_models_V2H.md)                                   | (416,128)   | Depth                        | PyTorch              | 9477ms                       | 110ms                          |
| [ConvNeXt atto](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                    | (224,224)   | Classification               | pytorch-image-models | 898ms                        | 139ms                          |
| [ConvNeXt femto](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                   | (224,224)   | Classification               | pytorch-image-models | 1086ms                       | 166ms                          |
| [ConvNeXt femto ols](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                               | (224,224)   | Classification               | pytorch-image-models | 1129ms                       | 163ms                          |
| [CSP-Darknet](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (256,256)   | Classification               | pytorch-image-models | 5808ms                       | 31ms                           |
| [CSP-ResNet](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                      | (256,256)   | Classification               | pytorch-image-models | 4409ms                       | 21ms                           |
| [CSP-ResNeXt](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                     | (224,224)   | Classification               | pytorch-image-models | 6660ms                       | 167ms                          |
| [Darknet-53](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                       | (228,228)   | Classification               | pytorch-image-models | 13468ms                      | 8ms                            |
| [Darknet-aa53](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                     | (228,228)   | Classification               | pytorch-image-models | 16905ms                      | 7ms                            |
| [DenseNet121](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 5411ms                       | 27ms                           |
| [DenseNet161](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 11326ms                      | 58ms                           |
| [DenseNet169](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 6885ms                       | 36ms                           |
| [DenseNet201](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 9780ms                       | 47ms                           |
| [DenseNet Blur 121d](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                               | (224,224)   | Classification               | pytorch-image-models | 5532ms                       | 32ms                           |
| [DLA46x_c](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                        | (224,224)   | Classification               | pytorch-image-models | 538ms                        | 47ms                           |
| [DLA60x_c](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                        | (224,224)   | Classification               | pytorch-image-models | 686ms                        | 56ms                           |
| [DPN68](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                            | (224,224)   | Classification               | pytorch-image-models | 5058ms                       | 276ms                          |
| [DPN68b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                           | (224,224)   | Classification               | pytorch-image-models | 5227ms                       | 233ms                          |
| [ECA-ResNet101d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                  | (224,224)   | Classification               | pytorch-image-models | 19984ms                      | 151ms                          |
| [ECA-ResNet26t](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (320,320)   | Classification               | pytorch-image-models | 9412ms                       | 92ms                           |
| [ECA-ResNet50d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 9620ms                       | 93ms                           |
| [ECA-ResNet50t](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (320,320)   | Classification               | pytorch-image-models | 17755ms                      | 165ms                          |
| [ECA-ResNetlight](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                 | (224,224)   | Classification               | pytorch-image-models | 10632ms                      | 68ms                           |
| [EfficientNet Edge Large](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                          | (300,300)   | Classification               | pytorch-image-models | 10898ms                      | 7ms                            |
| [pruned EfficientNet Edge Large](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                   | (240,240)   | Classification               | pytorch-image-models | 7784ms                       | 4ms                            |
| [EfficientNet Edge Medium](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                         | (300,300)   | Classification               | pytorch-image-models | 4801ms                       | 5ms                            |
| [EfficientNet Edge Small](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                          | (224,224)   | Classification               | pytorch-image-models | 3457ms                       | 2ms                            |
| [pruned EfficientNet Edge Small](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 3438ms                       | 2ms                            |
| [EfficientNet Lite0](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                               | (224,224)   | Classification               | pytorch-image-models | 468ms                        | 2ms                            |
| [ESE-VoVNet 19-dw](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                 | (224,224)   | Classification               | pytorch-image-models | 2508ms                       | 16ms                           |
| [ESE-VoVNet 39b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                   | (224,224)   | Classification               | pytorch-image-models | 15937ms                      | 22ms                           |
| [FBNet-C](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                          | (224,224)   | Classification               | pytorch-image-models | 436ms                        | 2ms                            |
| [FBNetV3-B](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                       | (256,256)   | Classification               | pytorch-image-models | 786ms                        | 59ms                           |
| [FBNetV3-D](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                       | (256,256)   | Classification               | pytorch-image-models | 917ms                        | 66ms                           |
| [FBNetV3-G](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                       | (288,288)   | Classification               | pytorch-image-models | 3363ms                       | 97ms                           |
| [Global Context Resnet50t (gcresnet50t)](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                          | (256,256)   | Classification               | pytorch-image-models | 5137ms                       | 121ms                          |
| [GPU-Efficient ResNet Large (gernet_l)](./how_to_convert/How_to_convert_timm_models_V2H.md)                                            | (256,256)   | Classification               | pytorch-image-models | 5023ms                       | 6ms                            |
| [GPU-Efficient ResNet Middle (gernet_m)](./how_to_convert/How_to_convert_timm_models_V2H.md)                                           | (224,224)   | Classification               | pytorch-image-models | 4640ms                       | 4ms                            |
| [GPU-Efficient ResNet Small (gernet_s)](./how_to_convert/How_to_convert_timm_models_V2H.md)                                            | (224,224)   | Classification               | pytorch-image-models | 719ms                        | 2ms                            |
| [GhostNet-1.0x](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 199ms                        | 59ms                           |
| [(Gluon) ResNet101 v1b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 19880ms                      | 7ms                            |
| [(Gluon) ResNet101 v1c](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 19926ms                      | 7ms                            |
| [(Gluon) ResNet101 v1d](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 19939ms                      | 8ms                            |
| [(Gluon) ResNet101 v1s](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 20562ms                      | 8ms                            |
| [(Gluon) ResNet152 v1b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 29738ms                      | 9ms                            |
| [(Gluon) ResNet152 v1c](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 29829ms                      | 9ms                            |
| [(Gluon) ResNet152 v1d](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 29808ms                      | 10ms                           |
| [(Gluon) ResNet152 v1s](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                            | (224,224)   | Classification               | pytorch-image-models | 30428ms                      | 10ms                           |
| [(Gluon) ResNet18 v1b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                             | (224,224)   | Classification               | pytorch-image-models | 4439ms                       | 3ms                            |
| [(Gluon) ResNet34 v1b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                             | (224,224)   | Classification               | pytorch-image-models | 10042ms                      | 4ms                            |
| [(Gluon) ResNet50 v1b](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                             | (224,224)   | Classification               | pytorch-image-models | 9550ms                       | 4ms                            |
| [(Gluon) ResNet50 v1c](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                             | (224,224)   | Classification               | pytorch-image-models | 9609ms                       | 5ms                            |
| [(Gluon) ResNet50 v1d](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                             | (224,224)   | Classification               | pytorch-image-models | 9593ms                       | 5ms                            |
| [(Gluon) ResNet50 v1s](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                             | (224,224)   | Classification               | pytorch-image-models | 10228ms                      | 6ms                            |
| [(Gluon) ResNeXt101 32x4d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                        | (224,224)   | Classification               | pytorch-image-models | 14368ms                      | 325ms                          |
| [(Gluon) ResNeXt101 64x4d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                        | (224,224)   | Classification               | pytorch-image-models | 15130ms                      | 567ms                          |
| [(Gluon) SE-ResNeXt101 32-4d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                     | (224,224)   | Classification               | pytorch-image-models | 14470ms                      | 421ms                          |
| [(Gluon) SE-ResNeXt101 64-4d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                     | (224,224)   | Classification               | pytorch-image-models | 15359ms                      | 659ms                          |
| [(Gluon) SE-ResNeXt50 32-4d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                      | (224,224)   | Classification               | pytorch-image-models | 7701ms                       | 229ms                          |
| [(Gluon) Xception65](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (299,299)   | Classification               | pytorch-image-models | T.B.D                        | 73ms                           |
| [HardcoreNAS_A](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 238ms                        | 22ms                           |
| [HardcoreNAS_B](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 273ms                        | 16ms                           |
| [HardcoreNAS_C](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 288ms                        | 19ms                           |
| [HardcoreNAS_D](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 329ms                        | 40ms                           |
| [HardcoreNAS_E](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 363ms                        | 40ms                           |
| [HardcoreNAS_F](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 365ms                        | 40ms                           |
| [HRNet W18](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 8777ms                       | 8ms                            |
| [HRNet W18small](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                   | (224,224)   | Classification               | pytorch-image-models | 3324ms                       | 4ms                            |
| [HRNet W18small V2](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                | (224,224)   | Classification               | pytorch-image-models | 5537ms                       | 6ms                            |
| [HRNet W30](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 7941ms                       | 11ms                           |
| [HRNet W32](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 15130ms                      | 10ms                           |
| [HRNet W40](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 11756ms                      | 14ms                           |
| [HRNet W44](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 9988ms                       | 16ms                           |
| [HRNet W48](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 24360ms                      | 17ms                           |
| [Instagram ResNeXt101 32x8 WSL](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                   | (224,224)   | Classification               | pytorch-image-models | 15741ms                      | 927ms                          |
| [Inception ResNet v2](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                             | (224,224)   | Classification               | pytorch-image-models | 10388ms                      | 555ms                          |
| [PP-LCNet-0.5x](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                    | (224,224)   | Classification               | pytorch-image-models | 119ms                        | 9ms                            |
| [PP-LCNet-0.75x](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                   | (224,224)   | Classification               | pytorch-image-models | 207ms                        | 10ms                           |
| [PP-LCNet-1x](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 408ms                        | 11ms                           |
| [(Legacy) SE-ResNet-152](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                          | (224,224)   | Classification               | pytorch-image-models | 29178ms                      | 240ms                          |
| [(Legacy) SE-ResNet-18](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                           | (224,224)   | Classification               | pytorch-image-models | 4451ms                       | 23ms                           |
| [(Legacy) SE-ResNet-34](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                           | (224,224)   | Classification               | pytorch-image-models | 10043ms                      | 43ms                           |
| [(Legacy) SE-ResNet-50](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                           | (224,224)   | Classification               | pytorch-image-models | 8936ms                       | 95ms                           |
| [(Legacy) SE-ResNeXt-26](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                          | (224,224)   | Classification               | pytorch-image-models | 4264ms                       | 123ms                          |
| [MnasNet-B1 depth multiplier 1.0](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                  | (224,224)   | Classification               | pytorch-image-models | 463ms                        | 2ms                            |
| [MnasNet-Small depth multiplier 1.0](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                              | (224,224)   | Classification               | pytorch-image-models | 151ms                        | 17ms                           |
| [MobileNet V2 with channel multiplier of 0.5](./how_to_convert/How_to_convert_timm_models_V2H.md)                                      | (224,224)   | Classification               | pytorch-image-models | 193ms                        | 1ms                            |
| [MobileNet V2 with channel multiplier of 1.0](./how_to_convert/How_to_convert_timm_models_V2H.md)                                      | (256,256)   | Classification               | pytorch-image-models | 446ms                        | 1ms                            |
| [MobileNet V2 with channel multiplier of 1.1](./how_to_convert/How_to_convert_timm_models_V2H.md)                                      | (224,224)   | Classification               | pytorch-image-models | 595ms                        | 2ms                            |
| [MobileNet V2 with channel multiplier of 1.2](./how_to_convert/How_to_convert_timm_models_V2H.md)                                      | (320,320)   | Classification               | pytorch-image-models | 776ms                        | 2ms                            |
| [MobileNet V2 with channel multiplier of 1.4](./how_to_convert/How_to_convert_timm_models_V2H.md)                                      | (416,416)   | Classification               | pytorch-image-models | 721ms                        | 3ms                            |
| [MobileNet V3 Large 1.0](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                          | (224,224)   | Classification               | pytorch-image-models | 227ms                        | 21ms                           |
| [MobileNet V3 Large 1.0,  21k pretraining](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                        | (224,224)   | Classification               | pytorch-image-models | 424ms                        | 22ms                           |
| [MobileNet V3 (RW variant)](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                       | (224,224)   | Classification               | pytorch-image-models | 230ms                        | 22ms                           |
| [MobileNet V3 Small 0.5](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                          | (224,224)   | Classification               | pytorch-image-models | 56ms                         | 17ms                           |
| [MobileNet V3 Small 0.75](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                         | (224,224)   | Classification               | pytorch-image-models | 78ms                         | 17ms                           |
| [MobileNet V3 Small 1.0](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                          | (224,224)   | Classification               | pytorch-image-models | 90ms                         | 17ms                           |
| [RegNetX 200MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 193ms                        | 36ms                           |
| [RegNetX 400MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 256ms                        | 75ms                           |
| [RegNetX 600MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 769ms                        | 104ms                          |
| [RegNetX 800MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 1247ms                       | 102ms                          |
| [RegNetX 1.6GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 1286ms                       | 219ms                          |
| [RegNetX 3.2GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 5536ms                       | 570ms                          |
| [RegNetX 4.0GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 2819ms                       | 524ms                          |
| [RegNetX 6.4GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 4075ms                       | 877ms                          |
| [RegNetY 200MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 197ms                        | 41ms                           |
| [RegNetY 400MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 529ms                        | 66ms                           |
| [RegNetY 600MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 976ms                        | 94ms                           |
| [RegNetY 800MF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 761ms                        | 108ms                          |
| [RegNetY 1.6GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 1252ms                       | 255ms                          |
| [RegNetY 4.0GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 6269ms                       | 780ms                          |
| [RegNetY 8.0GF](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (224,224)   | Classification               | pytorch-image-models | 15149ms                      | 982ms                          |
| [RepVGG-A2](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 4803ms                       | 22ms                           |
| [RepVGG-B0](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 9784ms                       | 22ms                           |
| [RepVGG-B1](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 43972ms                      | 32ms                           |
| [RepVGG-B1g4](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 22580ms                      | 732ms                          |
| [RepVGG-B2](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 49365ms                      | 40ms                           |
| [RepVGG-B3](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 25982ms                      | 47ms                           |
| [Res2Net-101 26w×4s](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (224,224)   | Classification               | pytorch-image-models | 6500ms                       | 206ms                          |
| [Res2Net-50 14w×8s](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                               | (224,224)   | Classification               | pytorch-image-models | 5045ms                       | 225ms                          |
| [Res2Net-50 26w×4s](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                               | (224,224)   | Classification               | pytorch-image-models | 4285ms                       | 115ms                          |
| [Res2Net-50 26w×6s](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                               | (224,224)   | Classification               | pytorch-image-models | 5843ms                       | 174ms                          |
| [Res2Net-50 48w×2s](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                               | (224,224)   | Classification               | pytorch-image-models | 5300ms                       | 86ms                           |
| [Res2Next-50](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                     | (224,224)   | Classification               | pytorch-image-models | 7616ms                       | 242ms                          |
| [ResNeSt-14](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                      | (224,224)   | Classification               | pytorch-image-models | 3095ms                       | 632ms                          |
| [ResNeSt-26](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                      | (224,224)   | Classification               | pytorch-image-models | 4225ms                       | 833ms                          |
| [ResNeSt-50 1s4×24d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (224,224)   | Classification               | pytorch-image-models | 4359ms                       | 494ms                          |
| [ResNeSt-50 4s2×40d](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (224,224)   | Classification               | pytorch-image-models | 4537ms                       | 676ms                          |
| [ResNet-101](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                       | (224,224)   | Classification               | pytorch-image-models | 19904ms                      | 7ms                            |
| [ResNet-101-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                     | (320,320)   | Classification               | pytorch-image-models | 35090ms                      | 14ms                           |
| [ResNet-10-T](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 2078ms                       | 2ms                            |
| [ResNet-14-T](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 3184ms                       | 3ms                            |
| [ResNet-152](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                       | (224,224)   | Classification               | pytorch-image-models | 29760ms                      | 9ms                            |
| [ResNet-152-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                     | (320,320)   | Classification               | pytorch-image-models | 53734ms                      | 19ms                           |
| [ResNet-18](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 4442ms                       | 3ms                            |
| [ResNet-18-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 4496ms                       | 3ms                            |
| [ResNet-200-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                     | (320,320)   | Classification               | pytorch-image-models | 75269ms                      | 27ms                           |
| [ResNet-26](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 5181ms                       | 3ms                            |
| [ResNet-26-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 5224ms                       | 4ms                            |
| [ResNet-26-T](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (256,256)   | Classification               | pytorch-image-models | 3371ms                       | 5ms                            |
| [ResNet-34](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 10033ms                      | 4ms                            |
| [ResNet-34-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 10100ms                      | 4ms                            |
| [ResNet-50](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                        | (224,224)   | Classification               | pytorch-image-models | 9545ms                       | 4ms                            |
| [ResNet-50-D](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 9596ms                       | 5ms                            |
| [ResNet-50 avgpool anti-aliasing](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                  | (288,288)   | Classification               | pytorch-image-models | 12685ms                      | 10ms                           |
| [ResNet-50 blur anti-aliasing](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                    | (224,224)   | Classification               | pytorch-image-models | 13123ms                      | 21ms                           |
| [ResNet-RS-101](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (288,288)   | Classification               | pytorch-image-models | 12318ms                      | 246ms                          |
| [ResNet-RS-152](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                   | (320,320)   | Classification               | pytorch-image-models | 53977ms                      | 241ms                          |
| [ResNet-RS-50](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                                    | (224,224)   | Classification               | pytorch-image-models | 9692ms                       | 95ms                           |
| [ResNet101-v2](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                     | (224,224)   | Classification               | pytorch-image-models | 19813ms                      | 26ms                           |
| [ResNet50-v2](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                      | (224,224)   | Classification               | pytorch-image-models | 9521ms                       | 15ms                           |
| [ResNeXt-101(32x8d)](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (224,224)   | Classification               | pytorch-image-models | 15744ms                      | 921ms                          |
| [ResNeXt-101(64x4d)](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (288,288)   | Classification               | pytorch-image-models | 22288ms                      | 850ms                          |
| [ResNeXt-50(32x4d)](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                               | (224,224)   | Classification               | pytorch-image-models | 7630ms                       | 175ms                          |
| [ResNeXt-50d(32x4d)](./how_to_convert/How_to_convert_timm_models_V2H_exceptional_case.md)                                              | (224,224)   | Classification               | pytorch-image-models | 7665ms                       | 175ms                          |
| [SelecSLS42_B](./how_to_convert/How_to_convert_timm_models_V2H.md)                                                                     | (224,224)   | Classification               | pytorch-image-models | 2812ms                       | 6ms                            |
| [Vision Transformer(base)](./how_to_convert/How_to_convert_VIT_onnx_models_V2H.md)                                                     | (224,224)   | Classification               | ONNX                 | 269936ms                     | 790ms                          |
| [Vision Transformer(small)](./how_to_convert/How_to_convert_VIT_onnx_models_V2H.md)                                                    | (224,224)   | Classification               | ONNX                 | 68628ms                      | 400ms                          |
| [Vision Transformer(tiny)](./how_to_convert/How_to_convert_VIT_onnx_models_V2H.md)                                                     | (224,224)   | Classification               | ONNX                 | 18704ms                      | 226ms                          |
| [Swin Transformer(small)](./how_to_convert/How_to_convert_Swin_onnx_models_V2H.md)                                                     | (224,224)   | Classification               | ONNX                 | 130646ms                     | 1478ms                         |
| [Swin Transformer(tiny)](./how_to_convert/How_to_convert_Swin_onnx_models_V2H.md)                                                      | (224,224)   | Classification               | ONNX                 | 67137ms                      | 1114ms                         |
| [MiDaS v2.1 Small](https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt)                                     | (256,256)   | Depth Estimation             | ONNX                 | 3900ms                       | 55ms                           |
| [FCN HRNet W18](https://bj.bcebos.com/paddleseg/dygraph/pascal_voc12/fcn_hrnetw18_voc12aug_512x512_40k/model.pdparams)                 | (512,512)   | Semantic Segmentation        | ONNX                 | 24510ms                      | 229ms                          |


[^1]: DRP-AI TVM is powered by EdgeCortix MERA™ Compiler Framework.  
[^2]: Linux Package Version AISDK v5.20,  DRP-AI Support Package Version AISDK v5.20,DRP-AI TVM Version v2.5.0, DRP-AI Translator Version i8 v1.04  
[^3]: Linux Package Version AISDK v5.00,  DRP-AI Support Package Version AISDK v5.00  
[^4]: [Expand DRP-AI memory area 512MB to 2GB](./how_to_expand_drpai_memory.md)
