# How to obtain profiling data

This page explains about profiling function of DRP-AI TVM[^1].

## Index
- [Run Profiler](#run-profiler)  
- [Extract Subgraph Information](#extract-subgraph-information)  

## Run Profiler
### Get Profiling Data
DRP-AI TVM[^1] has profiling funcion, which shows the processing time for each subgraph assigned to CPU/DRP-AI.
It can be run in the application that runs on the target board, i.e., [Application Example](../../../apps).

To use the profiling function, call `ProfileRun()` function in the application source code instead of `Run()` function, which is normal inference function.  
Folllowing is example code.  
```cpp
...

runtime.SetInput(i, input.data());

/*Run inference and generate profiling result*/
runtime.ProfileRun("profile_table.txt", "profile.csv");

```
This function will generate following two files.  Filename can be any string with file extension.
1. `profile_table.txt` : Text file contains processing time for each subgraph assigned to CPU/DRP-AI.  
2. `profile.csv`       : Same contents as above in CSV format.  

Following is example of `profile_table.txt` generated from ResNet18 (ONNX Model Zoo).
```sh
Name                                      Duration (us)  Percent                            Argument Shapes  Device  Count  
tvmgen_default_tvmgen_default_mera_drp_0       20858.46    98.97  float32[1, 3, 224, 224], float16[1, 1000]    cpu0      1  
----------                                                                                                                  
Sum                                            20858.46    98.97                                                         1  
Total                                          21075.17                                                        cpu0      1  
```

Following is example of `profile.csv` generated from ResNet18 (ONNX Model Zoo).
```sh
Percent,Argument Shapes,Duration (us),Name,Count,Device
98.9717,"float32[1, 3, 224, 224], float16[1, 1000]",20858.5,"tvmgen_default_tvmgen_default_mera_drp_0",1,"cpu0"
```


### Analyze Profiling Data
Profiling data can be analyzed by the script `profile_reader.py`, which is provided in this directory.  
The script will show following data.  
- Total inference processing time  
- Number of subgraphs in CPU/DRP-AI processing  
- Percentage of CPU/DRP-AI processing  
- Processing time for CPU/DRP-AI.  

Run the script with the `profile_table.txt` generated by the `ProfileRun()` function.  
Command is as follows.  
```sh
python3 profile_reader.py <profile_table.txt>
```

For example, running the script with ResNet18 ONNX profiling data will show following result.  
```sh
root@rzv2ma:~# python3 profile_reader.py profile_table.txt
------------------------------------------------
[SUMMARY]
  CPU     sub-graph num :  0
  DRP-AI  sub-graph num :  1
  CPU     inference time :     0.00 [msec] ( 0.0[%])
  DRP-AI  inference time :    17.59 [msec] (100.0[%])
  Total inference time   :    17.59 [msec]
------------------------------------------------
```

## Extract Subgraph Information
DRP-AI TVM[^1] generates subgraph in Relay and assign them to CPU/DRP-AI to be computed.  
This subgraph information can be obtained when compiling the AI model.
As default, it will be printed on console.
To save the subgraph information into a file, please modify the compiling script, i.e., [`compile_onnx_model.py`](../../../tutorials) as below.  

#### Before : compile_onnx_model.py L78~L85
```py
...
# 3.2.2 Run backend compiler
drp.build(mod, \
               params, \
               "arm", \
               drp_config_runtime, \
               output_dir=output_dir, \
               disable_concat = opts["disable_concat"]
               )

```

#### After
```py
...
import io
with io.StringIO() as iostr:
    sys.stdout = iostr
    drp.build(mod, \
               params, \
               "arm", \
               drp_config_runtime, \
               output_dir=output_dir, \
               disable_concat = opts["disable_concat"]
               )
    build_log = iostr.getvalue()
    sys.stdout = sys.__stdout__
with open(output_dir+"/relay_log.txt","w") as f:
    f.writelines(build_log)
```

The modified script will generate `<PREFIX>/relay_log.txt`.  
For example, `relay_log.txt` for ResNet18 (ONNX Model Zoo) will be as follows.
```py
def @main(%data: Tensor[(1, 3, 224, 224), float32]) -> Tensor[(1, 1000), float16] {
  @tvmgen_default_tvmgen_default_mera_drp_0(%data)
}

def @tvmgen_default_tvmgen_default_mera_drp_0(%mera_drp_0_i0: Tensor[(1, 3, 224, 224), float32], Inline=1, Compiler="mera_drp", global_symbol="tvmgen_default_tvmgen_default_mera_drp_0", Primitive=1) -> Tensor[(1, 1000), float16] {
...
```
This shows that in ResNet18, there is only single subgraph, which is called `tvmgen_default_tvmgen_default_mera_drp_0`.


[^1]: DRP-AI TVM is powered by EdgeCortix MERAâ„¢ Compiler Framework.
