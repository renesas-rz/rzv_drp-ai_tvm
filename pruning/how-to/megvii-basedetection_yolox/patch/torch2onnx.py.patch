diff --git tools/export_onnx.py tools/torch2onnx.py
index 3bcbb1d..715ec52 100644
--- tools/export_onnx.py
+++ tools/torch2onnx.py
@@ -1,6 +1,42 @@
 #!/usr/bin/env python3
 # -*- coding:utf-8 -*-
+#######################################################################################################################
+# DISCLAIMER
+# This software is supplied by Renesas Electronics Corporation and is only intended for use with Renesas products. No
+# other uses are authorized. This software is owned by Renesas Electronics Corporation and is protected under all
+# applicable laws, including copyright laws.
+# THIS SOFTWARE IS PROVIDED "AS IS" AND RENESAS MAKES NO WARRANTIES REGARDING
+# THIS SOFTWARE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. ALL SUCH WARRANTIES ARE EXPRESSLY DISCLAIMED. TO THE MAXIMUM
+# EXTENT PERMITTED NOT PROHIBITED BY LAW, NEITHER RENESAS ELECTRONICS CORPORATION NOR ANY OF ITS AFFILIATED COMPANIES
+# SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES FOR ANY REASON RELATED TO
+# THIS SOFTWARE, EVEN IF RENESAS OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+# Renesas reserves the right, without notice, to make changes to this software and to discontinue the availability of
+# this software. By using this software, you agree to the additional terms and conditions found by accessing the
+# following link:
+# http://www.renesas.com/disclaimer
+#
+# Copyright (C) 2023 Renesas Electronics Corporation. All rights reserved.
+#######################################################################################################################
 # Copyright (c) Megvii, Inc. and its affiliates.
+#######################################################################################################################
+#    Copyright (c) 2021-2022 Megvii Inc. All rights reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+#######################################################################################################################
+# Description : Export pytorch model to onnx format.
+# Version history:
+#   - Support the DRP-AI Extension Pack
 
 import argparse
 import os
@@ -13,6 +49,13 @@ from yolox.exp import get_exp
 from yolox.models.network_blocks import SiLU
 from yolox.utils import replace_module
 
+# Modified by Renesas
+from drpai_compaction_tool.pytorch import load_pruned_state_dict, \
+                                          get_model_info
+# decode in inference or not
+DECODE_IN_INFERENCE = False
+# onnx opset version
+OPSET_VERSION = 11
 
 def make_parser():
     parser = argparse.ArgumentParser("YOLOX onnx deploy")
@@ -25,9 +68,6 @@ def make_parser():
     parser.add_argument(
         "--output", default="output", type=str, help="output node name of onnx model"
     )
-    parser.add_argument(
-        "-o", "--opset", default=11, type=int, help="onnx opset version"
-    )
     parser.add_argument("--batch-size", type=int, default=1, help="batch size")
     parser.add_argument(
         "--dynamic", action="store_true", help="whether the input shape should be dynamic or not"
@@ -49,11 +89,9 @@ def make_parser():
         default=None,
         nargs=argparse.REMAINDER,
     )
-    parser.add_argument(
-        "--decode_in_inference",
-        action="store_true",
-        help="decode in inference or not"
-    )
+    # Modified by Renesas
+    parser.add_argument('--is_pruned_weight', action='store_true',
+                        help='When converting the model with pruned weight, please specify this option')
 
     return parser
 
@@ -81,14 +119,23 @@ def main():
     model.eval()
     if "model" in ckpt:
         ckpt = ckpt["model"]
-    model.load_state_dict(ckpt)
+	# Modified by Renesas
+    if args.is_pruned_weight:
+        # Load the pruned weight
+        load_pruned_state_dict(model, ckpt)
+        # Confirming the result of pruning.
+        print(get_model_info(model, [(args.batch_size, 3, exp.test_size[0], exp.test_size[1])]))
+    else:
+        model.load_state_dict(ckpt)
     model = replace_module(model, nn.SiLU, SiLU)
-    model.head.decode_in_inference = args.decode_in_inference
+    # Modified by Renesas
+    model.head.decode_in_inference = DECODE_IN_INFERENCE
 
     logger.info("loading checkpoint done.")
     dummy_input = torch.randn(args.batch_size, 3, exp.test_size[0], exp.test_size[1])
 
-    torch.onnx._export(
+    # Modified by Renesas
+    torch.onnx.export(
         model,
         dummy_input,
         args.output_name,
@@ -96,7 +143,7 @@ def main():
         output_names=[args.output],
         dynamic_axes={args.input: {0: 'batch'},
                       args.output: {0: 'batch'}} if args.dynamic else None,
-        opset_version=args.opset,
+        opset_version=OPSET_VERSION,
     )
     logger.info("generated onnx model named {}".format(args.output_name))
 
