diff --git tools/eval.py tools/eval_with_pruned_model.py
index 83ad76b..c7bc755 100644
--- tools/eval.py
+++ tools/eval_with_pruned_model.py
@@ -1,6 +1,42 @@
 #!/usr/bin/env python3
 # -*- coding:utf-8 -*-
+#######################################################################################################################
+# DISCLAIMER
+# This software is supplied by Renesas Electronics Corporation and is only intended for use with Renesas products. No
+# other uses are authorized. This software is owned by Renesas Electronics Corporation and is protected under all
+# applicable laws, including copyright laws.
+# THIS SOFTWARE IS PROVIDED "AS IS" AND RENESAS MAKES NO WARRANTIES REGARDING
+# THIS SOFTWARE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. ALL SUCH WARRANTIES ARE EXPRESSLY DISCLAIMED. TO THE MAXIMUM
+# EXTENT PERMITTED NOT PROHIBITED BY LAW, NEITHER RENESAS ELECTRONICS CORPORATION NOR ANY OF ITS AFFILIATED COMPANIES
+# SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES FOR ANY REASON RELATED TO
+# THIS SOFTWARE, EVEN IF RENESAS OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+# Renesas reserves the right, without notice, to make changes to this software and to discontinue the availability of
+# this software. By using this software, you agree to the additional terms and conditions found by accessing the
+# following link:
+# http://www.renesas.com/disclaimer
+#
+# Copyright (C) 2023 Renesas Electronics Corporation. All rights reserved.
+#######################################################################################################################
 # Copyright (c) Megvii, Inc. and its affiliates.
+#######################################################################################################################
+#    Copyright (c) 2021-2022 Megvii Inc. All rights reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+#######################################################################################################################
+# Description : Test the YOLOX-s with onnx format.
+# Version history:
+#   - Support the DRP-AI Extension Pack
 
 import argparse
 import os
@@ -23,6 +59,10 @@ from yolox.utils import (
     setup_logger
 )
 
+# Modified by Renesas
+from drpai_compaction_tool.pytorch import load_pruned_state_dict
+from drpai_compaction_tool.pytorch import get_model_info as drpai_get_model_info
+
 
 def make_parser():
     parser = argparse.ArgumentParser("YOLOX Eval")
@@ -75,12 +115,12 @@ def make_parser():
         action="store_true",
         help="Fuse conv and bn for testing.",
     )
+    # Modified by Renesas
     parser.add_argument(
-        "--trt",
-        dest="trt",
+        "--use_pytorch_model",
         default=False,
         action="store_true",
-        help="Using TensorRT model for testing.",
+        help="Using PyTorch model for testing.",
     )
     parser.add_argument(
         "--legacy",
@@ -109,6 +149,9 @@ def make_parser():
         default=None,
         nargs=argparse.REMAINDER,
     )
+    # Modified by Renesas
+    parser.add_argument('--is_pruned_weight', action='store_true',
+                        help='When testing the model with pruned weight, please specify this option')
     return parser
 
 
@@ -123,6 +166,9 @@ def main(exp, args, num_gpu):
         )
 
     is_distributed = num_gpu > 1
+    # Modified by Renesas
+    print("Warning: Not using distributed mode")
+    is_distributed = False
 
     # set environment variables for distributed training
     configure_nccl()
@@ -149,7 +195,12 @@ def main(exp, args, num_gpu):
     logger.info("Model Summary: {}".format(get_model_info(model, exp.test_size)))
     logger.info("Model Structure:\n{}".format(str(model)))
 
-    evaluator = exp.get_evaluator(args.batch_size, is_distributed, args.test, args.legacy)
+    # Modified by Renesas
+    if args.use_pytorch_model:
+        evaluator = exp.get_evaluator(args.batch_size, is_distributed, args.test, args.legacy)
+    else:
+        evaluator = exp.get_onnx_evaluator(args.batch_size, is_distributed, args.test, args.legacy)
+
     evaluator.per_class_AP = True
     evaluator.per_class_AR = True
 
@@ -157,7 +208,7 @@ def main(exp, args, num_gpu):
     model.cuda(rank)
     model.eval()
 
-    if not args.speed and not args.trt:
+    if not args.speed and args.use_pytorch_model:
         if args.ckpt is None:
             ckpt_file = os.path.join(file_name, "best_ckpt.pth")
         else:
@@ -165,7 +216,17 @@ def main(exp, args, num_gpu):
         logger.info("loading checkpoint from {}".format(ckpt_file))
         loc = "cuda:{}".format(rank)
         ckpt = torch.load(ckpt_file, map_location=loc)
-        model.load_state_dict(ckpt["model"])
+
+        # Modified by Renesas
+        if args.is_pruned_weight:
+            # Load the pruned weight
+            load_pruned_state_dict(model, ckpt["model"])
+            # Confirming the result of pruning.
+            INPUT_SHAPE = (1,3,640,640)
+            print(drpai_get_model_info(model, [INPUT_SHAPE]))
+        else:
+            model.load_state_dict(ckpt["model"])
+
         logger.info("loaded checkpoint done.")
 
     if is_distributed:
@@ -175,23 +236,24 @@ def main(exp, args, num_gpu):
         logger.info("\tFusing model...")
         model = fuse_model(model)
 
-    if args.trt:
+    # Modified by Renesas
+    if not args.use_pytorch_model:
         assert (
             not args.fuse and not is_distributed and args.batch_size == 1
-        ), "TensorRT model is not support model fusing and distributed inferencing!"
-        trt_file = os.path.join(file_name, "model_trt.pth")
+        ), "ONNX model is not support model fusing and distributed inferencing!"
+        onnx_file = args.ckpt
         assert os.path.exists(
-            trt_file
-        ), "TensorRT model is not found!\n Run tools/trt.py first!"
+            onnx_file
+        ), "ONNX model is not found!\n Export onnx model from pth file first!"
         model.head.decode_in_inference = False
         decoder = model.head.decode_outputs
     else:
-        trt_file = None
+        onnx_file = None
         decoder = None
 
     # start evaluate
     *_, summary = evaluator.evaluate(
-        model, is_distributed, args.fp16, trt_file, decoder, exp.test_size
+        model, is_distributed, args.fp16, onnx_file, decoder, exp.test_size
     )
     logger.info("\n" + summary)
 
