diff --git yolox/evaluators/voc_evaluator.py yolox/evaluators/onnx_voc_evaluator.py
index b5052e7..b196c1c 100644
--- yolox/evaluators/voc_evaluator.py
+++ yolox/evaluators/onnx_voc_evaluator.py
@@ -1,22 +1,54 @@
 #!/usr/bin/env python3
 # -*- coding:utf-8 -*-
+#######################################################################################################################
+# DISCLAIMER
+# This software is supplied by Renesas Electronics Corporation and is only intended for use with Renesas products. No
+# other uses are authorized. This software is owned by Renesas Electronics Corporation and is protected under all
+# applicable laws, including copyright laws.
+# THIS SOFTWARE IS PROVIDED "AS IS" AND RENESAS MAKES NO WARRANTIES REGARDING
+# THIS SOFTWARE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. ALL SUCH WARRANTIES ARE EXPRESSLY DISCLAIMED. TO THE MAXIMUM
+# EXTENT PERMITTED NOT PROHIBITED BY LAW, NEITHER RENESAS ELECTRONICS CORPORATION NOR ANY OF ITS AFFILIATED COMPANIES
+# SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES FOR ANY REASON RELATED TO
+# THIS SOFTWARE, EVEN IF RENESAS OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+# Renesas reserves the right, without notice, to make changes to this software and to discontinue the availability of
+# this software. By using this software, you agree to the additional terms and conditions found by accessing the
+# following link:
+# http://www.renesas.com/disclaimer
+#
+# Copyright (C) 2023 Renesas Electronics Corporation. All rights reserved.
+#######################################################################################################################
 # Copyright (c) Megvii, Inc. and its affiliates.
+#######################################################################################################################
+#    Copyright (c) 2021-2022 Megvii Inc. All rights reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+#######################################################################################################################
+# Description : VOC evaluator with onnx format.
+# Version history:
+#   - Support the DRP-AI Extension Pack
 
-import sys
-import tempfile
 import time
-from collections import ChainMap
-from loguru import logger
+import onnxruntime as ort
 from tqdm import tqdm
-
-import numpy as np
-
 import torch
+from collections import ChainMap
 
+from yolox.evaluators.voc_evaluator import VOCEvaluator
 from yolox.utils import gather, is_main_process, postprocess, synchronize, time_synchronized
 
 
-class VOCEvaluator:
+class OnnxVOCEvaluator(VOCEvaluator):
     """
     VOC AP Evaluation class.
     """
@@ -38,19 +70,14 @@ class VOCEvaluator:
                 is defined in the config file.
             nmsthre (float): IoU threshold of non-max supression ranging from 0 to 1.
         """
-        self.dataloader = dataloader
-        self.img_size = img_size
-        self.confthre = confthre
-        self.nmsthre = nmsthre
-        self.num_classes = num_classes
-        self.num_images = len(dataloader.dataset)
+        super().__init__(dataloader, img_size, confthre, nmsthre, num_classes)
 
     def evaluate(
         self,
         model,
         distributed=False,
         half=False,
-        trt_file=None,
+        onnx_file=None,
         decoder=None,
         test_size=None,
     ):
@@ -81,15 +108,18 @@ class VOCEvaluator:
         nms_time = 0
         n_samples = max(len(self.dataloader) - 1, 1)
 
-        if trt_file is not None:
-            from torch2trt import TRTModule
-
-            model_trt = TRTModule()
-            model_trt.load_state_dict(torch.load(trt_file))
-
-            x = torch.ones(1, 3, test_size[0], test_size[1]).cuda()
+        # Modified by Renesas
+        if onnx_file is not None:
+            onnx_model = ort.InferenceSession(onnx_file, 
+                                              providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
+            self.input_name = onnx_model.get_inputs()[0].name
+            self.output_name = onnx_model.get_outputs()[0].name
+            device, = list(set(p.device for p in model.parameters()))
+            # Feed forward to make internal variable in model
+            x = torch.ones(1, 3, test_size[0], test_size[1]).to(device)
             model(x)
-            model = model_trt
+        else:
+            raise RuntimeError("Please set the `onnx_file` argument to path to ONNX file.")
 
         for cur_iter, (imgs, _, info_imgs, ids) in enumerate(
             progress_bar(self.dataloader)
@@ -102,7 +132,13 @@ class VOCEvaluator:
                 if is_time_record:
                     start = time.time()
 
-                outputs = model(imgs)
+                # Modified by Renesas
+                device = imgs.device
+                imgs = imgs.to('cpu').detach().numpy()
+                outputs = onnx_model.run([self.output_name], {self.input_name:imgs.tolist()})
+                outputs = torch.from_numpy(outputs[0])
+                outputs = outputs.to(device)
+
                 if decoder is not None:
                     outputs = decoder(outputs, dtype=outputs.type())
 
@@ -128,81 +164,3 @@ class VOCEvaluator:
         eval_results = self.evaluate_prediction(data_list, statistics)
         synchronize()
         return eval_results
-
-    def convert_to_voc_format(self, outputs, info_imgs, ids):
-        predictions = {}
-        for (output, img_h, img_w, img_id) in zip(
-            outputs, info_imgs[0], info_imgs[1], ids
-        ):
-            if output is None:
-                predictions[int(img_id)] = (None, None, None)
-                continue
-            output = output.cpu()
-
-            bboxes = output[:, 0:4]
-
-            # preprocessing: resize
-            scale = min(
-                self.img_size[0] / float(img_h), self.img_size[1] / float(img_w)
-            )
-            bboxes /= scale
-
-            cls = output[:, 6]
-            scores = output[:, 4] * output[:, 5]
-
-            predictions[int(img_id)] = (bboxes, cls, scores)
-        return predictions
-
-    def evaluate_prediction(self, data_dict, statistics):
-        if not is_main_process():
-            return 0, 0, None
-
-        logger.info("Evaluate in main process...")
-
-        inference_time = statistics[0].item()
-        nms_time = statistics[1].item()
-        n_samples = statistics[2].item()
-
-        a_infer_time = 1000 * inference_time / (n_samples * self.dataloader.batch_size)
-        a_nms_time = 1000 * nms_time / (n_samples * self.dataloader.batch_size)
-
-        time_info = ", ".join(
-            [
-                "Average {} time: {:.2f} ms".format(k, v)
-                for k, v in zip(
-                    ["forward", "NMS", "inference"],
-                    [a_infer_time, a_nms_time, (a_infer_time + a_nms_time)],
-                )
-            ]
-        )
-
-        info = time_info + "\n"
-
-        all_boxes = [
-            [[] for _ in range(self.num_images)] for _ in range(self.num_classes)
-        ]
-        for img_num in range(self.num_images):
-            bboxes, cls, scores = data_dict[img_num]
-            if bboxes is None:
-                for j in range(self.num_classes):
-                    all_boxes[j][img_num] = np.empty([0, 5], dtype=np.float32)
-                continue
-            for j in range(self.num_classes):
-                mask_c = cls == j
-                if sum(mask_c) == 0:
-                    all_boxes[j][img_num] = np.empty([0, 5], dtype=np.float32)
-                    continue
-
-                c_dets = torch.cat((bboxes, scores.unsqueeze(1)), dim=1)
-                all_boxes[j][img_num] = c_dets[mask_c].numpy()
-
-            sys.stdout.write(
-                "im_eval: {:d}/{:d} \r".format(img_num + 1, self.num_images)
-            )
-            sys.stdout.flush()
-
-        with tempfile.TemporaryDirectory() as tempdir:
-            mAP50, mAP70 = self.dataloader.dataset.evaluate_detections(
-                all_boxes, tempdir
-            )
-            return mAP50, mAP70, info
