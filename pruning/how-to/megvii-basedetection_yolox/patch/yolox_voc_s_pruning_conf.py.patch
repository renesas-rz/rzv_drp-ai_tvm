diff --git yolox_voc_s_pruning_conf.py yolox_voc_s_pruning_conf.py
new file mode 100644
index 0000000..acf422f
--- /dev/null
+++ yolox_voc_s_pruning_conf.py
@@ -0,0 +1,264 @@
+# encoding: utf-8
+#######################################################################################################################
+# DISCLAIMER
+# This software is supplied by Renesas Electronics Corporation and is only intended for use with Renesas products. No
+# other uses are authorized. This software is owned by Renesas Electronics Corporation and is protected under all
+# applicable laws, including copyright laws.
+# THIS SOFTWARE IS PROVIDED "AS IS" AND RENESAS MAKES NO WARRANTIES REGARDING
+# THIS SOFTWARE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. ALL SUCH WARRANTIES ARE EXPRESSLY DISCLAIMED. TO THE MAXIMUM
+# EXTENT PERMITTED NOT PROHIBITED BY LAW, NEITHER RENESAS ELECTRONICS CORPORATION NOR ANY OF ITS AFFILIATED COMPANIES
+# SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES FOR ANY REASON RELATED TO
+# THIS SOFTWARE, EVEN IF RENESAS OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+# Renesas reserves the right, without notice, to make changes to this software and to discontinue the availability of
+# this software. By using this software, you agree to the additional terms and conditions found by accessing the
+# following link:
+# http://www.renesas.com/disclaimer
+#
+# Copyright (C) 2023 Renesas Electronics Corporation. All rights reserved.
+#######################################################################################################################
+#######################################################################################################################
+#    Copyright (c) 2021-2022 Megvii Inc. All rights reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License");
+#    you may not use this file except in compliance with the License.
+#    You may obtain a copy of the License at
+#
+#        http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS,
+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#    See the License for the specific language governing permissions and
+#    limitations under the License.
+#######################################################################################################################
+# Description : YOLOX-s configurations.
+# Version history:
+#   - Support the DRP-AI Extension Pack
+
+import os
+import math
+import time
+from loguru import logger
+
+import torch
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.tensorboard import SummaryWriter
+
+from yolox.core import Trainer
+from yolox.data import DataPrefetcher
+from yolox.utils import (
+    ModelEMA,
+    WandbLogger,
+    get_model_info,
+    occupy_mem,
+    is_parallel,
+    load_ckpt
+)
+
+from exps.example.yolox_voc.yolox_voc_s import Exp as MyExp
+
+# Modified by Renesas
+from drpai_compaction_tool.pytorch import make_pruning_layer_list, Pruner, \
+                                          deepcopy_model, load_pruned_state_dict
+from drpai_compaction_tool.pytorch import get_model_info as drpai_get_model_info
+
+
+class PrunedModelEMA(ModelEMA):
+    def __init__(self, model, decay=0.9999, updates=0):
+        # Modified by Renesas
+        # Create EMA(FP32)
+        self.ema = deepcopy_model(model.module if is_parallel(model) else model).eval()
+        self.updates = updates
+
+        # decay exponential ramp (to help early epochs)
+        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))
+        for p in self.ema.parameters():
+            p.requires_grad_(False)
+
+
+class PruningTrainer(Trainer):
+    def __init__(self, exp, args):
+        super().__init__(exp, args)
+
+    def before_train(self):
+        logger.info("args: {}".format(self.args))
+        logger.info("exp value:\n{}".format(self.exp))
+
+        # model related init
+        torch.cuda.set_device(self.local_rank)
+        model = self.exp.get_model()
+        logger.info(
+            "Model Summary: {}".format(get_model_info(model, self.exp.test_size))
+        )
+        model.to(self.device)
+
+        # solver related init
+        self.optimizer = self.exp.get_optimizer(self.args.batch_size)
+
+        # value of epoch will be set in `resume_train`
+        model = self.resume_train(model)
+
+        # data related init
+        self.no_aug = self.start_epoch >= self.max_epoch - self.exp.no_aug_epochs
+        self.train_loader = self.exp.get_data_loader(
+            batch_size=self.args.batch_size,
+            is_distributed=self.is_distributed,
+            no_aug=self.no_aug,
+            cache_img=self.args.cache,
+        )
+        # Modified by Renesas
+        # To avoid the error that is related to DataLoader, this example waits for a while.
+        time.sleep(100)
+        logger.info("init prefetcher, this might take one minute or less...")
+        self.prefetcher = DataPrefetcher(self.train_loader)
+        # max_iter means iters per epoch
+        self.max_iter = len(self.train_loader)
+
+        self.lr_scheduler = self.exp.get_lr_scheduler(
+            self.exp.basic_lr_per_img * self.args.batch_size, self.max_iter
+        )
+        if self.args.occupy:
+            occupy_mem(self.local_rank)
+
+        if self.is_distributed:
+            model = DDP(model, device_ids=[self.local_rank], broadcast_buffers=False)
+
+        # Modified by Renesas
+        # Preparing for pruning the model
+        PRUNING_RATE = os.getenv("PRUNING_RATE", None)
+        if PRUNING_RATE is None:
+            raise RuntimeError("No environment variable. "
+                               "Before running this script, Please set environment variable(PRUNING_RATE)"
+                               "to the pruning rate. e.g. $export PRUNING_RATE=0.7")
+        else:
+            PRUNING_RATE = float(PRUNING_RATE)
+        inps, targets = self.prefetcher.next()
+        pruning_layer_list = make_pruning_layer_list(model.module if is_parallel(model) else model,
+                                                     input_data={"x":inps, "targets":targets})
+        print(f"pruning_layer_list = \n\t{pruning_layer_list}")
+        self.pruner = Pruner(model.module if is_parallel(model) else model, 
+                             pruning_layer_list=pruning_layer_list,
+                             final_pr=PRUNING_RATE)
+
+        print(drpai_get_model_info(model.module if is_parallel(model) else model,
+                                   input_data={"x":inps, "targets":targets}))
+
+        if self.use_model_ema:
+            # Modified by Renesas
+            self.ema_model = PrunedModelEMA(model,
+                                            decay=0.9998)
+            self.ema_model.updates = self.max_iter * self.start_epoch
+
+        self.model = model
+
+        self.evaluator = self.exp.get_evaluator(
+            batch_size=self.args.batch_size, is_distributed=self.is_distributed
+        )
+        # Tensorboard and Wandb loggers
+        if self.rank == 0:
+            if self.args.logger == "tensorboard":
+                self.tblogger = SummaryWriter(os.path.join(self.file_name, "tensorboard"))
+            elif self.args.logger == "wandb":
+                wandb_params = dict()
+                for k, v in zip(self.args.opts[0::2], self.args.opts[1::2]):
+                    if k.startswith("wandb-"):
+                        wandb_params.update({k.lstrip("wandb-"): v})
+                self.wandb_logger = WandbLogger(config=vars(self.exp), **wandb_params)
+            else:
+                raise ValueError("logger must be either 'tensorboard' or 'wandb'")
+
+        logger.info("Training start...")
+        logger.info("\n{}".format(model))
+
+    def before_iter(self):
+        # Updating the pruning parameters
+        self.pruner.update()
+
+    def resume_train(self, model):
+        if self.args.resume:
+            logger.info("resume training")
+            if self.args.ckpt is None:
+                ckpt_file = os.path.join(self.file_name, "latest" + "_ckpt.pth")
+            else:
+                ckpt_file = self.args.ckpt
+
+            ckpt = torch.load(ckpt_file, map_location=self.device)
+
+            # Modified by Renesas
+            # resume the model/optimizer state dict
+            IS_PRUNED_WEIGHT = os.getenv("IS_PRUNED_WEIGHT", None)
+            if IS_PRUNED_WEIGHT is None:
+                logger.info('loading the dense weight to model')
+                model.load_state_dict(ckpt["model"])
+            else:
+                logger.info('loading the pruned weight to model.')
+                load_pruned_state_dict(model, ckpt["model"])
+
+            self.optimizer.load_state_dict(ckpt["optimizer"])
+            self.best_ap = ckpt.pop("best_ap", 0)
+            # resume the training states variables
+            start_epoch = (
+                self.args.start_epoch - 1
+                if self.args.start_epoch is not None
+                else ckpt["start_epoch"]
+            )
+            self.start_epoch = start_epoch
+            logger.info(
+                "loaded checkpoint '{}' (epoch {})".format(
+                    self.args.resume, self.start_epoch
+                )
+            )  # noqa
+        else:
+            if self.args.ckpt is not None:
+                logger.info("loading checkpoint for fine tuning")
+                ckpt_file = self.args.ckpt
+                ckpt = torch.load(ckpt_file, map_location=self.device)["model"]
+
+                # Modified by Renesas
+                IS_PRUNED_WEIGHT = os.getenv("IS_PRUNED_WEIGHT", None)
+                if IS_PRUNED_WEIGHT is None:
+                    logger.info('loading the dense weight to model')
+                    model = load_ckpt(model, ckpt)
+                else:
+                    logger.info('loading the pruned weight to model')
+                    load_pruned_state_dict(model, ckpt)
+
+            self.start_epoch = 0
+
+        return model
+
+
+class Exp(MyExp):
+    def __init__(self):
+        super(Exp, self).__init__()
+        self.num_classes = 20
+        self.depth = 0.33
+        self.width = 0.50
+        self.warmup_epochs = 1
+
+        # ---------- transform config ------------ #
+        self.mosaic_prob = 1.0
+        self.mixup_prob = 1.0
+        self.hsv_prob = 1.0
+        self.flip_prob = 0.5
+
+        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(".")[0]
+
+    def get_trainer(self, args):
+        trainer = PruningTrainer(self, args)
+        # NOTE: trainer shouldn't be an attribute of exp object
+        return trainer
+
+    def get_onnx_evaluator(self, batch_size, is_distributed, testdev=False, legacy=False):
+        from onnx_voc_evaluator import OnnxVOCEvaluator
+
+        val_loader = self.get_eval_loader(batch_size, is_distributed, testdev, legacy)
+        evaluator = OnnxVOCEvaluator(
+            dataloader=val_loader,
+            img_size=self.test_size,
+            confthre=self.test_conf,
+            nmsthre=self.nmsthre,
+            num_classes=self.num_classes,
+        )
+        return evaluator
