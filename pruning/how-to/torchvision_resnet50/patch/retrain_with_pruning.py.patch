diff --git train.py retrain_with_pruning.py
index 00af630..3caeba2 100644
--- train.py
+++ retrain_with_pruning.py
@@ -1,3 +1,55 @@
+#######################################################################################################################
+# DISCLAIMER
+# This software is supplied by Renesas Electronics Corporation and is only intended for use with Renesas products. No
+# other uses are authorized. This software is owned by Renesas Electronics Corporation and is protected under all
+# applicable laws, including copyright laws.
+# THIS SOFTWARE IS PROVIDED "AS IS" AND RENESAS MAKES NO WARRANTIES REGARDING
+# THIS SOFTWARE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. ALL SUCH WARRANTIES ARE EXPRESSLY DISCLAIMED. TO THE MAXIMUM
+# EXTENT PERMITTED NOT PROHIBITED BY LAW, NEITHER RENESAS ELECTRONICS CORPORATION NOR ANY OF ITS AFFILIATED COMPANIES
+# SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES FOR ANY REASON RELATED TO
+# THIS SOFTWARE, EVEN IF RENESAS OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+# Renesas reserves the right, without notice, to make changes to this software and to discontinue the availability of
+# this software. By using this software, you agree to the additional terms and conditions found by accessing the
+# following link:
+# http://www.renesas.com/disclaimer
+#
+# Copyright (C) 2023 Renesas Electronics Corporation. All rights reserved.
+#######################################################################################################################
+#######################################################################################################################
+# BSD 3-Clause License
+#
+# Copyright (c) Soumith Chintala 2016, 
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+#
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+#
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#######################################################################################################################
+# Description : Retrain resnet50 by using oneshot pruning.
+# Version history:
+#   - Support the DRP-AI Extension Pack
 import datetime
 import os
 import time
@@ -14,8 +66,13 @@ from torch import nn
 from torch.utils.data.dataloader import default_collate
 from torchvision.transforms.functional import InterpolationMode
 
+# Modified by Renesas
+# Import the DRP-AI Extension Pack module
+from drpai_compaction_tool.pytorch import make_pruning_layer_list, \
+                                          Pruner, \
+                                          get_model_info
 
-def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema=None, scaler=None):
+def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema=None, scaler=None, pruner=None):
     model.train()
     metric_logger = utils.MetricLogger(delimiter="  ")
     metric_logger.add_meter("lr", utils.SmoothedValue(window_size=1, fmt="{value}"))
@@ -23,6 +80,10 @@ def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, arg
 
     header = f"Epoch: [{epoch}]"
     for i, (image, target) in enumerate(metric_logger.log_every(data_loader, args.print_freq, header)):
+        # Modified by Renesas
+        # Update the pruning parameters
+        if pruner:
+            pruner.update()
         start_time = time.time()
         image, target = image.to(device), target.to(device)
         with torch.cuda.amp.autocast(enabled=scaler is not None):
@@ -192,6 +253,8 @@ def main(args):
         utils.mkdir(args.output_dir)
 
     utils.init_distributed_mode(args)
+    # Modified by Renesas
+    _validate_args(args)
     print(args)
 
     device = torch.device(args.device)
@@ -326,8 +389,34 @@ def main(args):
         model_ema = utils.ExponentialMovingAverage(model_without_ddp, device=device, decay=1.0 - alpha)
 
     if args.resume:
-        checkpoint = torch.load(args.resume, map_location="cpu")
-        model_without_ddp.load_state_dict(checkpoint["model"])
+        # Modified by Renesas
+        if args.is_pruned_weight:
+            # Preparing for pruning the model with a pruning rate of 0.0
+            images, _ = next(iter(data_loader))
+            pruning_layer_list = make_pruning_layer_list(model, 
+                                                         input_data=[images.to(device)])
+            print(f"pruning_layer_list = \n\t{pruning_layer_list}")
+            _ = Pruner(model, 
+                       pruning_layer_list=pruning_layer_list,
+                       final_pr=0.0)
+
+            # Load pretrained pruned weights
+            print("Loading pretrained pruned weights ...")
+            checkpoint = torch.load(args.resume, map_location="cpu")
+            model_without_ddp.load_state_dict(checkpoint["model"])
+
+            # Confirming the result of loading weights correctly
+            print(get_model_info(model, input_data=[images.to(device)]))
+
+        else:
+            # Load pretrained weights
+            print("Loading pretrained model ...")
+            checkpoint = torch.load(args.resume, map_location="cpu")
+            model_without_ddp.load_state_dict(checkpoint["model"])
+
+        # Force reset of the resumed epoch to 0
+        checkpoint["epoch"] = -1
+
         if not args.test_only:
             optimizer.load_state_dict(checkpoint["optimizer"])
             lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
@@ -347,12 +436,25 @@ def main(args):
             evaluate(model, criterion, data_loader_test, device=device)
         return
 
+    # Modified by Renesas
+    # Preparing for pruning the model
+    images, _ = next(iter(data_loader))
+    pruning_layer_list = make_pruning_layer_list(model, input_data=[images.to(device)])
+    print(f"pruning_layer_list = \n\t{pruning_layer_list}")
+    pruner = Pruner(model, 
+                    pruning_layer_list=pruning_layer_list,
+                    final_pr=args.pruning_rate)
+
+    # Confirming the result of pruning.
+    print(get_model_info(model, input_data=[images.to(device)]))
+
     print("Start training")
     start_time = time.time()
     for epoch in range(args.start_epoch, args.epochs):
         if args.distributed:
             train_sampler.set_epoch(epoch)
-        train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema, scaler)
+        # Modified by Renesas
+        train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema, scaler, pruner)
         lr_scheduler.step()
         evaluate(model, criterion, data_loader_test, device=device)
         if model_ema:
@@ -383,7 +485,8 @@ def get_args_parser(add_help=True):
     parser = argparse.ArgumentParser(description="PyTorch Classification Training", add_help=add_help)
 
     parser.add_argument("--data-path", default="/datasets01/imagenet_full_size/061417/", type=str, help="dataset path")
-    parser.add_argument("--model", default="resnet18", type=str, help="model name")
+    # Modified by Renesas
+    parser.add_argument("--model", default="resnet50", type=str, help="model name")
     parser.add_argument("--device", default="cuda", type=str, help="device (Use cuda or cpu Default: cuda)")
     parser.add_argument(
         "-b", "--batch-size", default=32, type=int, help="images per gpu, the total batch size is $NGPU x batch_size"
@@ -505,9 +608,26 @@ def get_args_parser(add_help=True):
         "--ra-reps", default=3, type=int, help="number of repetitions for Repeated Augmentation (default: 3)"
     )
     parser.add_argument("--weights", default=None, type=str, help="the weights enum name to load")
+
+    # Modified by Renesas
+    # Pruning paramters
+    parser.add_argument("--pruning_rate", default=0.7, type=float, help="pruning rate. this parameter can be in the range [0,1).")
+    parser.add_argument('--is_pruned_weight', action='store_true',
+                        help='When testing the model with pruned weight, please specify this option')
     return parser
 
 
+# Modified by Renesas
+def _validate_args(args):
+    if args.model_ema is True:
+        raise NotImplementedError("This scirpt does not support `model_ema` option.")
+    if args.model not in ["resnet50"]:
+        raise NotImplementedError(f"This script does not support {args.model}.")
+    if args.distributed is True:
+        print("Warning: Not using distributed mode")
+        args.distributed = False
+
+
 if __name__ == "__main__":
     args = get_args_parser().parse_args()
-    main(args)
+    main(args)
\ No newline at end of file
